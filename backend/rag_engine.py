# rag_engine.py
import os
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from dotenv import load_dotenv

load_dotenv()

DB_PATH = "local_faiss_index"


class RAGService:
    def __init__(self):
        self.vector_store = None
        self._load_db()

    def _load_db(self):
        """Åaduje gotowÄ… bazÄ™ wektorowÄ… z dysku."""
        if not os.path.exists(DB_PATH):
            print(f"--- âš ï¸ OSTRZEÅ»ENIE: Nie znaleziono bazy '{DB_PATH}'! ---")
            print("Uruchom najpierw plik 'build_db.py', aby stworzyÄ‡ bazÄ™.")
            return

        print(f"--- ðŸ“‚ Åadowanie bazy wiedzy z '{DB_PATH}'... ---")
        embeddings = OpenAIEmbeddings()

        # allow_dangerous_deserialization=True jest wymagane dla lokalnych plikÃ³w FAISS
        # Jest to bezpieczne, o ile sam wygenerowaÅ‚eÅ› te pliki.
        self.vector_store = FAISS.load_local(
            DB_PATH,
            embeddings,
            allow_dangerous_deserialization=True
        )
        print("--- âœ… Baza wiedzy zaÅ‚adowana i gotowa do uÅ¼ycia. ---")

    def search(self, query: str) -> str:
        if not self.vector_store:
            return "BÅ‚Ä…d: Baza wiedzy nie jest dostÄ™pna (nie zostaÅ‚a zbudowana)."

        # Wyszukaj 2 najlepsze fragmenty
        results = self.vector_store.similarity_search(query, k=2)

        # ZÅ‚Ä…cz wyniki w jeden tekst
        context = "\n\n".join([doc.page_content for doc in results])
        print(context)
        return context


# Tworzymy instancjÄ™ gotowÄ… do importu
RAG_ENGINE = RAGService()